{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "305121cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9172ab4d",
   "metadata": {},
   "source": [
    "## Enter the appropriate variable values\n",
    "ddb_table_name='glueworkshop-lab3'\n",
    "\n",
    "region_name = '<region-name>' # For example for Ohio it is us-east-2 amd us-west-2 for Oregon.\n",
    "Have a look at [Regions and Zones](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-regions-availability-zones.html)\n",
    "\n",
    "s3_bucket_name='<bucket-name>' # Enter the bucket name you created earlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e12711",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddb_table_name='glueworkshop-lab3'\n",
    "region_name = 'us-xxxx-x' # For example for Ohio it is us-east-2 amd us-west-2 for Oregon.\n",
    "s3_bucket_name='glueworkshop-xxxxxxxxxxxx'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3caff71",
   "metadata": {},
   "source": [
    "## Create the dynamodb with appropriate read and write capacity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31406d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "# Get service resource\n",
    "dynamodb = boto3.resource('dynamodb', region_name=region_name)\n",
    "from pprint import pprint\n",
    "table_status = dynamodb.create_table(\n",
    "    TableName=ddb_table_name,\n",
    "    KeySchema=[{'AttributeName': 'uuid','KeyType': 'HASH'}],\n",
    "    AttributeDefinitions=[{'AttributeName': 'uuid','AttributeType': 'N'}],\n",
    "    ProvisionedThroughput={'ReadCapacityUnits': 500,'WriteCapacityUnits': 5000}\n",
    "    )\n",
    "# Wait until the table exists.\n",
    "table_status.meta.client.get_waiter('table_exists').wait(TableName=ddb_table_name)\n",
    "pprint(table_status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5470d68c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql.types import IntegerType, StringType\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "\n",
    "from pyspark.context import SparkContext\n",
    "from awsglue.context import GlueContext\n",
    "from awsglue.dynamicframe import DynamicFrame\n",
    "\n",
    "glueContext = GlueContext(SparkContext.getOrCreate())\n",
    "\n",
    "import os\n",
    "from datetime import datetime\n",
    "from pycountry_convert import (\n",
    "    convert_country_alpha2_to_country_name,\n",
    "    convert_country_alpha2_to_continent,\n",
    "    convert_country_name_to_country_alpha2,\n",
    "    convert_country_alpha3_to_country_alpha2,\n",
    ")\n",
    "\n",
    "def get_country_code2(country_name):\n",
    "    country_code2 = 'US'\n",
    "    try:\n",
    "        country_code2 = convert_country_name_to_country_alpha2(country_name)\n",
    "    except KeyError:\n",
    "        country_code2 = ''\n",
    "    return country_code2\n",
    "\n",
    "\n",
    "udf_get_country_code2 = udf(lambda z: get_country_code2(z), StringType())\n",
    "\n",
    "s3_bucket = os.path.join('s3://', s3_bucket_name)\n",
    "df = spark.read.load(os.path.join(s3_bucket,\"input/lab2/sample.csv\"), \n",
    "                     format=\"csv\", \n",
    "                     sep=\",\", \n",
    "                     inferSchema=\"true\", \n",
    "                     header=\"true\")\n",
    "new_df = df.withColumn('country_code_2', udf_get_country_code2(col(\"Country\")))\n",
    "\n",
    "new_df_dyf=DynamicFrame.fromDF(new_df, glueContext, \"new_df_dyf\")\n",
    "\n",
    "print(\"Start writing to DBB : {}\".format(datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")))\n",
    "glueContext.write_dynamic_frame_from_options(\n",
    "    frame=new_df_dyf,\n",
    "    connection_type=\"dynamodb\",\n",
    "    connection_options={\n",
    "        \"dynamodb.output.tableName\": ddb_table_name,\n",
    "        \"dynamodb.throughput.write.percent\": \"1.0\"\n",
    "    }\n",
    ")\n",
    "print(\"Finished writing to DBB : {}\".format(datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "026ccc12",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8001a63b",
   "metadata": {},
   "source": [
    "## Delete the DynamoDB table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f240c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "dynamodb = boto3.resource('dynamodb', region_name=region_name)\n",
    "table = dynamodb.Table(ddb_table_name)\n",
    "table.delete()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Sparkmagic (PySpark)",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
